{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASSANDRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerrequisitos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder proceder con la instalación de Cassandra primero es necesario disponer de Java 8. Como vamos a emplear Ubuntu 16.04 como sistema operativo vamos a ver una forma de satisfacer este prerrequisito.<br><br>Abrir un terminal y ejecutar:<br><br><b><i>$ sudo apt install openjdk-8-jdk</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez finalizada dicha instalación, podremos comprobar la versión instalada de java ejecutando el comando:<br><br><b><i>$ java -version</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además para poder ejecutar la herramienta <b>cqlsh</b> de la que hablaremos posteriormente es necesario contar con Python 2.7. Para ello en esta máquina virtual se utiliza el entorno de Anaconda que puede descargarse desde la url: <a src=\"https://www.anaconda.com/distribution/\">https://www.anaconda.com/distribution/</a> . Una vez descargado el script Anaconda2-xxx.sh hay que darle persmisos de ejecución mediante el comando: <br><br><b><i> $ sudo chmod +x Anaconda2-xxx.sh </i></b><br><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso siguiente será ejecutar dicho script para proceder con la instalación de Anaconda: <br><br><b><i> $ ./Anaconda2-xxx.sh </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También habrá que añadir en el fichero <b>.bashrc </b> situado en el directorio /home del usuario osboxes de la máquina virtual la variable de entorno PYTHONPATH=/usr/lib/python2.7/dist-packages/. Para ello, se editará dicho fichero empleando, por ejemplo, gedit desde el directorio /home del usuario osboxes:<br><br><b><i> $ gedit .bashrc </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se abra el editor de texto se añadirá la siguiente línea al final del fichero y se guardará el cambio realizado: <br><br><b><i> export PYTHONPATH=/usr/lib/python2.7/dist-packages/ </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que el sistema operativo cargue dichos cambios habrá que cerrar la terminal y volverla a abrir o forzar a que se vuelva a cargar el fichero .bashrc mediante el comando: <br><br><b><i> $ source .bashrc </i></b><br><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede comprobar la versión de Python disponible mediante el comando: <br><br><b><i> $ python --version </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación a partir de los ficheros binarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso será descargar el fichero comprimido que contiene los binarios desde la url: <b><a src=\"http://cassandra.apache.org/download/\">http://cassandra.apache.org/download/</a></b>. A continuación, se podrá descomprimir mediante el comando: <br><br><b><i> $ tar -xzvf apache-cassandra-3.x-bin.tar.gz </i></b><br><br> En la ruta <b>apache-cassandra-3.x-bin/bin</b> se encuentran los ficheros binarios del paquete de Cassandra descargado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es posible terminar contando con varias versiones de Cassandra, una posibilidad es crear un enlace simbólico llamado por ejemplo cassandra_latest que apunte a la última versión descargada:<br><br><b><i> $ ln -s apache-cassandra-3.x-bin cassandra_latest </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esa forma a través de la ruta <b>cassandra_latest/bin</b> se podrá acceder a los binarios de la última versión de Cassandra descargada. <br><br> Una práctica bastante útil es añadir la ruta apache-cassandra-3.x-bin/bin a la variable de entorno PATH del fichero .bashrc comentado anteriormente para poder invocar a cualquiera de los binarios presentes en la carpeta bin desde cualquier ubicación en la terminal. Para ello, se editará el fichero .bashrc desde el directorio /home/osboxes y se añadirá la línea: <br><br><b><i> export PATH=\"/home/osboxes/ruta_donde_se_haya_descomprimido_el_tar_gz/apache-cassandra-3.x-bin/bin:$PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ejecutar Cassandra en \"foreground\" mediante el comando: <br><br><b><i> $ bin/cassandra -f </i></b><br><br> Si se ejecuta de esta forma, se puede detener mediante Ctrl+c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se puede lanzar Cassandra en modo \"background\" empleando el comando: <br><br><b><i> $ bin/cassandra </i></b><br><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso para detener Cassandra, en primer lugar habrá que ver el PID del proceso Cassandra mediante el comando: <br><br><b><i> $ ps aux | grep CassandraDaemon </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Y posteriormente matar dicho proceso mediante el comando: <br><br><b><i> $ sudo kill -9 pid_obtenido_en_paso_anterior</i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación a partir de paquetes de Debian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este método lo primero que hay que hacer es descargar las claves GPG para poder acceder al repositorio de Apache Cassandra mediante el comando: <br><br><b><i> $ wget -q -O - https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add - </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso siguiente consiste en añadir al sistema el repositorio de Casandra: <br><br><b><i> sudo sh -c 'echo \"deb http://www.apache.org/dist/cassandra/debian 311x main\" > /etc/apt/sources.list.d/cassandra.list' </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se actualizará la lista de paquetes disponibles y se procederá a instalar Cassandra: <br><br><b><i> $ sudo apt-get update </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i> $ sudo apt-get install cassandra </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El servicio de Cassandra arrancará automáticamente una vez finalice la instalación. También se puede arrancar y parar dicho servicio con las 2 siguientes instrucciones respectivamente: <br><br><b><i> $ sudo service cassandra start </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i> $ sudo service cassandra stop </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede hacer que el servicio de Cassandra arranque automáticamente en cada inicio del sistema ejecutando: <br><br><b><i> sudo update-rc.d cassandra defaults </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a la configuración de Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si Cassandra se ha descargado en modo de fichero comprimido 'tar' que contiene sus binarios entonces los ficheros de configuración se encontrarán en el directorio <b>conf/</b>. En caso de que Cassandra se haya instalado a partir de paquetes los ficheros de configuración estarán en <b>/etc/cassandra/</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchas propiedades de configuración se pueden especificar a través de propiedades del fichero <b>cassandra.yaml</b> situado por defecto en alguna de las 2 rutas anteriores. Entre esas propiedades se encuentran las siguientes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <b>cluster_name </b>: [cluster_name] Permite especificar el nombre del cluster. Todos los miembros deben tener el mismo nombre de cluster.<br><br>\n",
    "   <b>storage_port </b>: [port] Puerto TCP para comandos y datos. Si se cambia su valor hay que recordar que el Firewall no lo bloquee.<br><br>\n",
    "   <b>seeds </b>: [ip_node], [ip_node], ..., [ip_node] Permite establecer la listas de IPs separadadas por comas de los servers del cluster que actuarán como seeds. Es decir, nodos conocidos donde se puede encontrar información sobre el cluster como qué nodos lo forman. No constituyen un single point of failure, ya que todos los nodos tienen esta información pero se trata de ubicaciones conocidas a las que pueden acceder nuevos nodos.<br><br>\n",
    "   <b>listen_address </b>: [ip_node] Indica la IP del nodo actual que se empleará para comunicación interna entre los nodos de Cassandra.<br><br>\n",
    "   <b>rpc_address </b>: [ip_node] Es la IP en la que escuchará esta instancia de Cassandra para su comunicación con el cliente empleando el protocolo CQL.<br><br>\n",
    "   <b>num_tokens </b>: [num] Es el número de VNodes en esta instanacia de Cassandra. Permite trabajar con particiones de datos y distribuirlos por el cluster.<br><br><b>endpoint_snitch </b>: Indica el snitch usado por Cassandra. Un snitch es el encargado de decir a Cassandra a qué Data Center y a qué Rack pertenece este nodo.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las configuraciones relativas a la JVM como el tamaño de heap se especifican en el fichero <b>cassandra_env.sh</b>. Se pueden pasar argumentos adicionales a la JVM a través de la variable de entorno <b>JVM_OPTS</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las propiedades asociadas al registro de información en logs se pueden indicar en el fichero <b>logback.xml</b>. Las notificaciones de log a nivel INFO (cuando se ejecuta Cassandra en modo \"foreground\" el nivel de log será INFO) por defecto se volcarán en el fichero <b>system.log</b> y las del modo DEBUG en el fichero <b>debug.log</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que se hayan guardado las modificaciones habrá que reiniciar el servicio de Cassandra mediante el comando: <br><br><b><i>$ sudo service cassandra restart </i></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a la arquitectura de Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Cassandra es un sistema distribuido en forma de anillo de nodos. También es descentralizado, es decir, todos los nodos son idénticos por lo que no existe el concepto de nodo maestro / nodo esclavo.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un resumen de la jeraquía de elementos de Casandra puede verse a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La unidad individual de datos recibe el nombre de <b>partición</b>. Cada partición está replicada en diferentes nodos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada copia de una partición recibe el nombre de <b>réplica</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un <b>Cluster</b> es un conjunto de Data Centers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un <b>Data Center</b> es un conjunto de Racks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un <b>Rack</b> es un conjunto de Servers de forma que ninguna réplica se almacena de forma redundante dentro del mismo rack asegurando que las réplicas se distribuyen en diferentes racks por si un rack tiene algún fallo. Todos los Servers del Rack están conectados al switch de red de dicho Rack que a su vez está conectado al Cluster al que pertenece. Además todos los Servers del Rack tienen una fuente de alimentación común. Si un Rack falla, por ejemplo, debido a algún problema en su switch o en su fuente de alimentación, ningún Server del Rack estará accesible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un <b>Server</b> es una instancia del software de Cassandra instalado en una máquina física o en una máquina virtual. Cada Server contiene un número de Virtual Nodes o VNodes (256 por defecto).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un <b>VNode</b> o nodo implementa una capa de almacenamiento de datos dentro de un Server de foma que dicha capa se corresponderá con un rango de tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/cassandrarf3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/vnodes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Datos de Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de datos de Cassandra está formado por keyspaces, column families, partition keys, column keys y column values. Para comenzar a explicar dichos componentes lo mejor es tratar de compararlos con sus homólogos en un modelo de datos relacional.<br><br>* Un <b>Keyspace</b> podría compararse con una base de datos.<br><br>* Una <b>column family</b> podría verse como una tabla.<br><br>* Una <b>partition key</b> de una fila con una parte o toda la primary key de una tabla. Permite agrupar filas relacionadas y actúa como una clave de búsqueda en una estructura clave-valor.<br><br>* Una <b>column key</b> con el column name de una tabla.<br><br>* El <b>column value</b> sería similar al concepto de column value de una base de datos relacional. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/partitions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se comentó anteriormente, Cassandra organiza los datos en particiones y cada una de ellas está formada por una o más filas con una o varias columnas. Cada nodo de un cluster es el encargado de almacenar un subconjunto del conjunto total de particiones.<br><br> Cuando se insertan registros, Cassandra calculará el hash (<b>token</b>) del valor de la partition key de los datos insertados, de forma que dicho hash servirá para conocer qué nodo será el encargado de almacenar esos datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podremos ver una <b>column family</b> como un map de una partition key a un map ordenado de pares (column key, column-value). El <b>partition key</b> funciona como el valor de búsqueda dentro de una column family.<br><br><b>Map(Partition_key, SortedMap((column_key_1, column_value_1), ...(column_key_n, column_value_n)))</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicación de Datos en Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mantener un registro de los nodos de un Cluster, Cassandra emplea el protocolo Gossip y los Snitches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Gossip</b> es un protocolo de comunicación para descubrir, compartir localización e información con otros nodos del cluster. Un nodo \"initiator\" del cluster elige aleatoriamente otro nodo \"peer\" para hablar con él. El nodo initiator envía al peer metadatos sobre sí mismo y sobre otros nodos de los que tiene información y recibe los metadatos que tiene el nodo peer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/gossip.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El <b>Snitch</b> determina a qué Rack y a qué Data Center pertenece un nodo concreto y en función de eso se obtiene dónde habrá que colocar las réplicas. Emplearemos <b>GossipingPropertyFileSnitch</b> (esta elección se especificará en cassandra.yaml) que utiliza la información del Data Center y del Rack presentes en el fichero <b>cassandra-rackdc.properties</b> del nodo local para difundir esta información a otros nodos empleando Gossip. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cassandra coloca las réplicas basándose en el Partitioner, en la Estrategia de Replicación y en el Factor de Replicación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un <b>Partitioner</b> es la función hash que se emplea para calcular el token de una partition key. El valor del token determinará  dónde se distribuye cada fila de datos. De forma que, por ejemplo, las filas cuyo valor de partition key está dentro de un rango de 1 a 20 estarán en el nodo A, las que estén en el rango 21-40 residirán en el nodo B, y así sucesivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestran 2 tipos de Partitioners:<br><br>*<b>Murmur3Partitioner</b>: es el partitioner por defecto. Se encarga de distribuir los datos uniformemente por el cluster empleando la función MurmurHash que asigna a cada partition key un hash de 64 bits en un rango de -2^63 a (2^63)-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<b>RandomPartitioner</b>: emplea la función hash MD5 para distribuir los datos uniformemente por el cluster en un rango de 0 a (2^127)-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El <b>Factor de Replicación</b> es el número de nodos del cluster que reciben copias de los mismos datos. Por ejemplo, si el factor de replicación es 3 entonces cada partición estará en 3 nodos distintos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 2 tipos de <b>Estrategias de Replicación</b>:<br><br>*<b>Simple Strategy</b>: esta estrategia se usa cuando se va a realizar el despliegue en un único Data Center y coloca la primera réplica en el nodo asignado por el partitioner y las siguientes réplicas se colocarán en los nodos más próximos siguiendo el sentido de las agujas del reloj. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<b>Network Topology Strategy</b>: permite definir cuántas réplicas se distribuyen en cada Data Center, de forma, que se trata de evitar que 2 réplicas se encuentren en el mismo rack. El hecho de hacer esto es para conseguir que si un rack se cae, haya otro rack del Data Center en el que se encuentre la réplica disponible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Más componentes de Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<b>Commit Log</b>: cada nodo de Cassandra mantiene un fichero de log secuencial donde se lleva el seguimiento de las escrituras a disco por razones de asegurar la integridad de los datos. Dichas escrituras son indexadas y escritas en una estructura conocida como Memtable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<b>Memtable</b>: permite cachear en memoria las operaciones de escritura con el resultado de su  ejecución. Por defecto se almacena en la zona de Heap de la JVM (Java Virtual Machine) pero se puede especificar otra zona de memoria, para por ejemplo, rebajar la carga de trabajo del Garbage Collection que se encarga de limpiar la memoria que no vaya a ser usada más de la zona de Heap de la JVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<b>SSTables</b>: cuando el commit log se llena, el contenido del memtable se vuelca a un fichero en disco llamado SSTables. Cuando finaliza este proceso tanto el commit log como el memtable se limpian y Cassandra particiona esas escrituras y distribuye sus réplicas por el cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<b>Compaction</b>: para mejorar la utilización de disco y el performance de lecturas, Cassandra periódicamente compacta varias SSTables haciendo un merge de claves, combinando columnas, etc, generando una única y nueva SSTable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear un cluster con 3 nodos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se verán los pasos para crear un cluster con 3 nodos en un mismo Data Center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisitos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*La misma versión de Cassandra debe estar instalado en los 3 nodos.<br><br>\\*No debe haber datos almacenados en ninguno de los 3 nodos. En caso de que existieran en alguna instancia habrá que pararla y borrar dichos datos de la siguiente forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>$ sudo service cassandra stop</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>$ sudo rm -rf /var/lib/cassandra/data/system/*</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*Debe haber conectividad entre los nodos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración de Cassandra.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ubicación por defecto de este fichero en una instalación empleando paquetes de Debian es <b>/etc/cassandra/cassandra.yaml</b>.<br><br> A continuación, se muestra un ejemplo de dicho fichero para el nodo 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster_name: 'Santander'<br>\n",
    "num_tokens: 256<br>\n",
    "seed_provider:<br>\n",
    "    \\- class_name: org.apache.cassandra.locator.SimpleSeedProvider<br>\n",
    "        \\- seeds: 192.168.0.100, 192.168.0.101<br>\n",
    "listen_address: 192.168.0.100<br> \n",
    "rpc_address: 192.168.0.100<br>\n",
    "endpoint_snitch: GossipingPropertyFileSnitch<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La configuración del fichero cassandra.yaml para el nodo 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster_name: 'Santander'<br>\n",
    "num_tokens: 256<br>\n",
    "seed_provider:<br>\n",
    "    \\- class_name: org.apache.cassandra.locator.SimpleSeedProvider<br>\n",
    "        \\- seeds: 192.168.0.100, 192.168.0.101<br>\n",
    "listen_address: 192.168.0.101<br> \n",
    "rpc_address: 192.168.0.101<br>\n",
    "endpoint_snitch: GossipingPropertyFileSnitch<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La configuración del fichero cassandra.yaml para el nodo 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster_name: 'Santander'<br>\n",
    "num_tokens: 256<br>\n",
    "seed_provider:<br>\n",
    "    \\- class_name: org.apache.cassandra.locator.SimpleSeedProvider<br>\n",
    "        \\- seeds: 192.168.0.100, 192.168.0.101<br>\n",
    "listen_address: 192.168.0.102<br> \n",
    "rpc_address: 192.168.0.102<br>\n",
    "endpoint_snitch: GossipingPropertyFileSnitch<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede comprobar en los ficheros anteriores, la lista de nodos seed es la misma en todos los nodos al igual que el nombre del cluster y el número de vnodes que habrá en cada nodo. Cada nodo emplea como listen_address y como rpc_address su propia IP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración de cassandra-rackdc.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ubicación por defecto de este fichero en una instalación empleando paquetes de Debian es <b>/etc/cassandra/cassandra-rackdc.properties</b>.<br><br>Se va a definir una configuración en la que los 3 nodos se encuentran en el mismo Data Center(que recibe el nombre de dc_Santander, hay que tener presente que Cassandra es case sensitive) pero 2 nodos estarán en el rack llamado rack1_Santander y el tercer nodo estará en el rack rack2_Santander.<br><br>A continuación, se muestra un ejemplo de dicho fichero para el nodo 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dc=dc_Santander<br>\n",
    "rack=rack1_Santander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La configuración del fichero cassandra-rackdc.properties para el nodo 2 será:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dc=dc_Santander<br>\n",
    "rack=rack1_Santander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La configuración del fichero cassandra-rackdc.properties para el nodo 3 será:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dc=dc_Santander<br>\n",
    "rack=rack2_Santander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrancar el cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poner en funcionamiento el cluster se procederá a arrancar primero los nodos que establecimos como seeds (en nuestro caso serían el nodo 1 y el nodo 2) en el fichero cassandra.yaml. Una vez que dichos nodos estén levantados se levantarán el resto de nodos del cluster.<br><br>Para arrancar un nodo, abriremos una terminal en dicho equipo y ejecutaremos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>$ sudo service cassandra start</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que se haya realizado ese paso en cada uno de los 3 nodos se puede comprobar el estado del cluster empleando una herramienta que veremos posteriormente llamada <b>nodetool</b>. Desde la terminal de cualquiera de los nodos ddel cluster ejecutar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>$ nodetool status</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida devolverá el nombre del dc (dc_Santander) y si todo ha ido bien se mostrará que el estado de cada uno de los 3 nodos es UN (Up and Normal) así como sus IPs, su carga (KB), su número de tokens (256 cada uno), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexión con el cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que el cluster esté arrancado, cada nodo dipondrá de la herramienta CQLSH desde donde podrá conectarse al cluster. Para ello, se abrirá una terminal y se ejecutará el comando cqlsh indicando alguna de las IPs donde Cassandra está escuchando por posibles clientes (IPs indicadas en la propiedad rpc_address del fichero cassandra.yaml):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>$ cqlsh 192.168.0.100</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Añadir un nodo a un cluster Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*Se instalará en el nodo que se quiere añadir la misma versión de Cassandra que tienen los nodos del cluster. Si se emplea la instalación a partir de paquetes de Debian se arranca automáticamente el servicio una vez instalado. Habrá que detener el servicio y limpiar los datos en ese nuevo nodo que se quiere añadir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*Dependiendo del snitch que se quiera utilizar se establecerán los parámetros adecuados en el fichero <b>cassandra-rackdc.properties</b>. En el apartado anterior se vió cómo proceder con el snitch GossipingPropertyFileSnitch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*Dentro del fichero <b>cassandra.yaml</b> habrá que tener en cuenta las siguientes propiedades:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Poner la opción <b>auto_bootstrap</b> a true. Consigue que el nuevo nodo consiga los datos adecuados automáticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Poner en <b>cluster_name</b> el nombre del cluster al que queremos unir el nuevo nodo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Especificar en la lista de <b>seeds</b> de la propiedad <b>seed_provider</b> alguno de los nodos que actúan como seed en el cluster. Un punto a tener en cuenta es que los nodos seed no pueden hacer bootstrap por lo que no hay que poner la IP del nodo que se quiere añadir en la lista de seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En la propiedad <b>num_tokens</b> especificar el mismo número de vnodes que en los demás servers del cluster (por ejemplo, 256). Los rangos de tokens se distribuyen proporcionalmente pero si las capacidades de HW varían se asigna un mayor rango de tokens a los sistemas con mayor rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Establecer el valor para la propiedad <b>endpoint_snitch</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Establecer la dirección IP para la propiedad <b>listen_address</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*Se arrancará el nodo bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*Se empleará <b>nodetool status</b> para comprobar que el nuevo nodo ha realizado el bootstrap correctamente y que los demás nodos están levantados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*Ejecutar <b>nodetool cleanup</b> en cada uno de los nodos del cluster excepto en el nuevo nodo. Este comando limpia los keyspaces y las partiton keys que ya no pertenecen al nodo donde se está ejecutando dicho comando. Es conveniente esperar a que la limpieza termine en el nodo donde se está ejecutando <b>nodetool cleanup</b> antes de intentar ejecutarlo en el siguiente nodo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
